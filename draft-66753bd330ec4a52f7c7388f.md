---
title: "A Practical Implementation of KANs."
slug: a-practical-implementation-of-kans

---

## TL;DR.

\[pending\]

> **Attributions:**
> 
> The [Anadigm 3.3volt QUADAPEX development kit](https://www.anadigm.com/apexquad-devkit.asp)***↗.***

---

## An Introduction.

KANs, or Kolmogorov–Arnold Networks, have the potential of addressing the hallucination problems associated with large language models, or LLMs:

> The purpose of this post is to describe the hardware that is used to run KANs.

---

## The Big Picture.

There are two worlds: The real, analog world and the sampled, digital world. For KANs to be effective, they need to operate in an analog space where functions are typically used to describe complex, overlapping frequencies. The functions that are used by KANs will need to be integrated, and differentiated, so that low-order complexities can be "pushed" into high-order space where "smoothed" frequencies can be accessed, deep-learning can be applied, and accurate inference can be generated.

---

## Prerequisites.

* The [Anadigm 3.3volt QUADAPEX development kit](https://www.anadigm.com/apexquad-devkit.asp)***↗.***
    
* An understanding of KANs, and
    
* An understanding of Mamba.
    

---

## What are Kolmogorov-Arnold Networks?

Kolmogorov-Arnold Networks, or KANs, are inspired by the Kolmogorov-Arnold Representation Theorem. KANs is a promising alternative to Multi-Layer Perceptrons, or MLPs. While MLPs have fixed activation functions on nodes (“neurons”), KANs have learnable activation functions on edges (“weights”). KANs have no linear weights at all – every weight parameter (as per MLPs) is replaced by a univariate function parametrized as a spline. KANs outperform MLPs in terms of accuracy and interpretability, can achieve better accuracy than larger MLPs in function fitting tasks, and possess faster neural scaling laws.

### The Digital Domain.

Digital solutions are used to build the current range of Large Language Models, or LLMs, but there are problems that are inherent to processing digital samples:

* Speed,
    
* Accuracy, and
    
* Power consumption.
    

### Speed.

Training LLMs is time-consuming. A two gigahertz (GHz) CPU can carry out two billion simple, single-cycle instructions per second. Of course, many complex instructions require multiple cycles to complete. For instance:

* Floating point add operations: 4 cycles,
    
* Floating point multiply operations: 7 cycles,
    
* Double precision multiply operations: 8 cycles, and
    
* Floating point divide operations: 23 cycles.
    

Given a 2GHz CPU, the processor can perform almost 87 million floating point divide operations per second. This means that each operation would take just under 12 nano seconds to complete:

> 2GHz = 0.5ns per cycle, therefore...  
> 0.5 x 23 cycles per each floating point divide operation *≈* 12ns for dividing approximated real numbers.

This sounds fast, but training an LLM requires dot product matrix calculations. A dot product matrix calculation requires far more cycles than a relatively simple floating point divide calculation.

Luckily, there are graphics processor units, or GPUs. As of the mid-2020s, top-of-the-range graphics processors can have tens of thousands of CUDA cores that can run multiple matrix operations in parallel. However, it *ultimately* doesn't matter how many cores a GPU has, because there is still another problem that digital samples cannot overcome when processing digital samples.

### Accuracy.

Floating point divide operations work on sampled data. This means that floating point numbers are real number *approximations*. Since real numbers can't be represented *exactly* in a fixed (i.e. digital) space, the results of floating point operations may not be fully accurate. This inaccuracy typically results in lost information which may, in turn, contribute to LLM hallucinations.

There are solutions that address the accuracy problems, but most of these fixes are used to mitigate specific errors when performing explicit operations. Sadly, one solution to the root cause of the problem is expensive: The width of digital elements, like registers in CPUs and memory cells in RAM, is a fixed space. Widening that space *might* work, but will be prohibitively expensive. Chip makers will not switch architectures if existing solutions solve most problems. Even *if* chip makers were to widen their existing 64-bit solutions, there is *still* no guarantee that a wider architecture would have enough resolution to generate accurate results.

Another contributing factor to LLM hallucinations is the size of the models. There is too much training data in these LLMs and the attention mechanisms get overwhelmed. This excess results in too many over-lapping vectors. Similarity searches that run on these vectors will find similar vectors with data paths that lead to the wrong result. Hallucinations happen far too often for current LLMs to be a reliable solution for scientists and engineers, let alone businesses and consumers.

These are difficult problems to solve. Difficult, but not impossible. It is time to find, and adopt, a better way of building, and using, LLMs. However, any solution will also need to address another problem with using digital samples.

### Power Consumption.

GPUs have played a significant role in modern AI development. They helped confirm the viability of using multi-layer perceptrons, or MLPs, as the foundation for building LLMs. However, GPUs consume a lot of energy. Some (unconfirmed) numbers show that 8% of the power in a data centre is used for training LLMs, while another 12% is spent on inference. This is a LOT of energy. My proposed hardware and software solution:

* Aims to reduce power consumption in data centres,
    
* Removes my reliance on using data centre models,
    
* Improves the accuracy of the local models I use, and
    
* Increases the speed of my own custom models.
    

I believe it is time to switch from the digital domain to the analog domain.

### The Analog Domain.

FPAAs, or Field Programmable *Analog* Arrays, are specialist chips that work with analog signals. A development board, called the [QuadApex Development Board](https://www.anadigm.com/apexquad-devkit.asp)↗, is currently available from from [Anadigm](https://www.anadigm.com/)***↗*** and comes with 4 x AN231E04 *dp*ASP (dynamically programmable analog signal processor) FPAA chips. I plan to use these chips as the hardware foundation for building, and running, next-generation LLMs.

Before looking at FPAAs, I first want to introduce FPGAs, or Field Programmable *Gate* Arrays. FPGAs are made of grids of logic gates that are programmable (AND, OR, NOT, etc.) and can be connected in any arrangement. These gates can be configured to emulate CPUs, GPUs, ROMs, etc. FPGAs are commonly used in consumer electronics, like digital cameras, where a firmware update to the device fixes any bugs and adds new features to the device. In other words, a firmware update is the process where the FPGA is reconfigured with new logic for the digital gates, and a new layout is defined for these gates to connect with each other.

Just like FPGAs that are made of programmable *digital* gates, FPAAs are made of grids of analog op-amps that are programmable (voltage follower, inverter, summing, etc.) and can be connected in any arrangement. These op-amps can be configured to emulate CPUs, GPUs, ROMs, etc. FPGAs are commonly used in consumer electronics, like digital cameras, where a firmware update to the device fixes any bugs and adds new features to the device. In other words, a firmware update is the process where the FPGA is reconfigured with new logic for the digital gates, and a new layout is defined for these gates to connect with each other.

Just like FPGAs that are made of programmable *digital* gates, an FPAA is made of a grids programmable *analog* op-amps that can be connected in any arrangement. These op-amps can be configured to emulate voltage followers, inverters, concept of a programmable chip is the same between these devices, but the main difference lies in their core elements: FPGAs use digital gates whereas FPAAs use analog op-amps.

Digital devices are very popular so there is a larger selection of FPGAs in the wild. FPAAs, on the other hand, are only used in specialist applications, like audio processing and video processing. As a result, FPAAs are less accessible, have fewer vendors, and are comparatively more expensive.

In my case, I will use FPAAs to build analog circuits that are more likely to yield the results I desire.

---

## Phase 1: Workbench.

I will conduct two experiments that demonstrate how I will use analog technologies to build the next generation of LLMs:

* The first experiment involves replacing discrete, logic-based gates (the current technology) with continuous, analog-based op-amps (an older technology), and
    
* The second experiment involves using floating-gate transistors, commonly used in flash memory, as the memory elements for storing the results of analog calculations. Flash memory experiments will begin in Phase II.
    

### The 1st Experiment: Op-Amps.

KANs are based on the Kolmogorov-Arnold Representation Theorem which, at first glance, looks very similar to the Universal Approximation Theorem used by Multi-Layer Perceptrons, or MLPs, which lay at the heart of current LLMs.

### Creating a new Base Model.

LLMs have abilities, or [emergent behaviours](https://www.assemblyai.com/blog/emergent-abilities-of-large-language-models/), that were (*probably*) not expected by the authors of a 2017 paper called [Attention is All You Need](https://arxiv.org/abs/2104.04692). The sudden appearance of these novel capabilities were exciting and, by November 2022, OpenAI had launched their killer app called ChatGPT. (This app had the fastest adoption rate, at the time, by reaching one million users within 2 months.)

However, everyone came to realise that LLMs have problems. As a result of the failings associated with LLMs, my base model takes a different approach: LAMs, or Large Algorithmic Models.

### LAM Training of Base Model.

LAMs are built using custom data, including:

* Axioms,
    
* First principles,
    
* Synthetic data for:
    
    * Languages,
        
    * Mathematics,
        
    * Engineering,
        
    * Science, and
        
* Battle-tested algorithms for LLMs.
    

### Axioms.

\[pending\]

### First Principles.

\[pending\]

### Synthetic Data.

\[pending\]

### Algorithms.

LLMs have proved the viability of the following algorithms, including:

### Embeddings.

\[pending\]

### Model Tuning.

\[pending\]

---

## Phase 2: Prototype.

\[pending\]

### The 2nd Experiment: Memory

\[pending\]

### RAM for Model Training.

\[pending\]

### Flash Memory for Model Storage.

Flash memory devices are built with the following manufacturing technologies:

* Floating gates, or
    
* Charge trap.
    

The floating gate cell uses polycrystalline silicon to provide a conductor for trapping the electrons. The charge trap cell uses non-conductive silicon nitride to provide an insulator.

Both technologies are used to build non-volatile memory elements in binary storage devices. These devices include (but are not limited to):

* SD cards,
    
* Thumb drives, and
    
* Solid state drives.
    

Flash memory elements have three pins:

* The source pin,
    
* The drain pin, and
    
* The gate pin.
    

\[pending\]

### Prototype Hardware.

\[pending\]

### The Other Components.

\[pending\]

### Designing a PCB Layout.

\[pending\]

### The Assembly.

\[pending\]

---

## Phase 3: Evaluation.

\[pending\]

### Three Computer Scientists.

Three LAMwareAI prototypes will be sent to the following computer scientists:

* Andrej Karpathy,
    
* Chris Lattner, and
    
* Ilya Sutskever.
    

### LAMwareAI Unboxing.

The LAMwareAI package includes the following:

* The LAMwareAI hardware,
    

\[pending\]

### LAMwareAI Boot Process.

When the LAMwareAI is plugged into a USB-enabled device, the following happens:

* A window opens on the device screen,
    
* The boot loader asks the user to choose the installation platform (NOTE: The boot loader already knows the answer to this question because it was able to open a screen on that device):
    
    * Windows,
        
    * Linux,
        
    * MacOS,
        
    * iOS, or
        
    * Android,
        
* The boot loader asks the user if this is the primary device for LAMwareAI,
    
* The boot loader asks the user if LAMwareAI will be installed on other devices,
    
* The boot loader installs the software and drivers for the platform, and
    
* The boot loader closes and the LAMwareAI software begins.
    

### LAMwareAI Software.

\[pending\]

---

## The Results.

\[pending\]

---

## In Conclusion.

\[pending\]

Until next time: Be safe, be kind, be awesome.

---

## Hash Tags.