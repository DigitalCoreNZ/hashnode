---
title: "NOTES: Kolmogorov–Arnold Networks."
slug: notes-kolmogorovarnold-networks

---

# TL;DR.

The following post contains a slightly edited version of a paper called:

[**KAN: Kolmogorov–Arnold Networks**](https://arxiv.org/abs/2404.19756)

> **Authors:**
> 
> Ziming Liu<sup>1,4∗</sup>, Yixuan Wang<sup>2</sup>, Sachin Vaidya<sup>1</sup>, Fabian Ruehle<sup>3,4</sup>, James Halverson<sup>3,4</sup>, Marin Soljačić<sup>1,4</sup>, Thomas Y. Hou<sup>2</sup>, and Max Tegmark<sup>1,4</sup>.

.

> **Institutions:**
> 
> <sup>1</sup> Massachusetts Institute of Technology,
> 
> <sup>2</sup> California Institute of Technology,
> 
> <sup>3</sup> Northeastern University, and
> 
> <sup>4</sup> The NSF Institute for Artificial Intelligence and Fundamental Interactions,

---

# An Introduction.

\[pending\]

> \[pending\]

---

# The Big Picture.

\[pending\]

---

# What are Kolmogorov-Arnold Networks?

Kolmogorov-Arnold Networks, or KANs, are inspired by the Kolmogorov-Arnold Representation Theorem. KANs are promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (“neurons”), KANs have learnable activation functions on edges (“weights”). KANs have no linear weights at all – every weight parameter (as per MLPs) is replaced by a univariate function parametrized as a spline. KANs outperform MLPs in terms of accuracy and interpretability. KANs can achieve better accuracy than larger MLPs in function fitting tasks. KANs possess faster neural scaling laws than MLPs.

## Multi-Layer Perceptrons.

MLPs are the foundational building blocks of today’s deep learning models. They are the default models in machine learning for approximating non-linear functions, due to their expressive power guaranteed by the Universal Approximation Theorem. But, are MLPs the best non-linear regressors we can build? After all, they have significant drawbacks. With transformers, MLPs consume almost all non-embedding parameters and are typically less interpretable (relative to attention layers) without post-analysis tools.

## Kolmogorov-Arnold Networks.

KANs have fully-connected structures, like MLPs. However, MLPs place fixed activation functions on nodes (“neurons”), while KANs place learnable activation functions on edges (“weights”). KANs have no linear weight matrices at all: instead, each weight parameter is replaced by a learn- able 1D function parametrized as a spline. KANs’ nodes simply sum incoming signals without applying any non-linearities. One might worry that KANs are hopelessly expensive, since each MLP's weight parameter becomes KAN's spline function. Fortunately, KANs usually allow much smaller computation graphs than MLPs.

Using Kolmogorov-Arnold Representation Theorem to build neural networks has already been studied. Many studies did not leverage more modern techniques (e.g., back propagation) to train the networks. A depth-2 width-(2n + 1) representation was investigated, with breaking of the curse of dimensionality observed both empirically and with an approximation theory given compositional structures of the function.

By generalizing the original Kolmogorov-Arnold Representation to arbitrary widths and depths, KANs have revitalized and contextualized Representation Theorem in today’s deep learning world, as well as using extensive empirical experiments to highlight its potential for AI + Science due to its accuracy and interpretability.

## The Best of Both Worlds.

KANs are nothing more than combinations of splines and MLPs. They leverage their respective strengths while avoiding their respective weaknesses. Splines are accurate for low-dimensional functions, easy to adjust locally, and able to switch between different resolutions. However, splines have a serious curse of dimensionality (COD) problem, because of their inability to exploit compositional structures. MLPs suffer less from COD thanks to their feature learning, but are less accurate than splines in low dimensions, because of their inability to optimize univariate functions.

To learn a function accurately, a model learn the compositional structure (external degrees of freedom), and also approximate well the univariate functions (internal degrees of freedom). KANs are such models since they have MLPs on the outside and splines on the inside. As a result, KANs can learn features (thanks to their external similarity to MLPs), and can also optimize these learned features to great accuracy (thanks to their internal similarity to splines).

Given a high dimensional function, splines would fail for large N due to COD; MLPs can potentially learn the generalized additive structure, but they are very inefficient for approximating the exponential and sine functions using ReLU activations. In contrast, KANs can learn both the compositional (generalized additive) structure and the univariate (exponential and sine) functions, hence outperforming MLPs by a large margin.

## Kolmogorov-Arnold Representation Theorem.

Andrey Kolmogorov and Vladimir Arnold established that if *f* is a multivariate continuous function on a bounded domain, then *f* can be written as a finite composition of continuous functions of a single variable and the binary operation of addition:

Specifically, for a smooth function:

> (2.1) *f* : \[0, 1\]<sup>n</sup> → R, where ϕ<sub>q,p</sub> : \[0, 1\] → R and Φ<sub>q</sub> : R → R.

They showed that the only true multivariate function is addition, since every other function can be written using univariate functions and sum. Learning a high-dimensional function *might* boil down to learning a polynomial number of 1D functions. However, these 1D functions can be non-smooth and even fractal, so they may not be learnable in practice, and the Kolmogorov-Arnold Representation Theorem was sentenced to death in machine learning, regarded as theoretically sound but practically useless.

There is now a more optimistic view about the usefulness of the Kolmogorov-Arnold Theorem for machine learning. We need not stick to the original formula above which has only two-layer non-linearities and a small number of terms (2n + 1) in the hidden layer. We can generalize the network to arbitrary widths and depths.

Most functions in science, and in daily life, are often smooth and have sparse compositional structures, potentially facilitating smooth Kolmogorov-Arnold Representations. Physicists often care more about typical cases rather than worst cases. Our physical world, and *specifically* machine learning tasks, must have structures to make physics, and machine learning, useful or generalizable.

## KAN architecture.

Suppose we have a supervised learning task consisting of input-output pairs {x<sub>i</sub> , y<sub>i</sub>}, where we want to find f such that y<sub>i</sub> ≈ f (x<sub>i</sub>) for all data points. The original equation implies that we are done if we can find appropriate univariate functions ϕ<sub>q,p</sub> and Φ<sub>q</sub>. We designed a neural network which explicitly parametrizes the equation.

Since all functions to be learned are univariate functions, we can parametrize each 1D function as a B-spline curve, with learnable coefficients of local B-spline basis functions.

Now we have a prototype of KAN, whose computation graph is exactly specified by the equation (with the input dimension n = 2), appearing as a two-layer neural network with activation functions placed on edges instead of nodes (simple summation is performed on nodes), and with width 2n + 1 in the middle layer.

Such a network is known to be too simple to approximate any function arbitrarily well in practice with smooth splines! We therefore generalize our KAN to be wider and deeper. It is not immediately clear how to make KANs deeper, since Kolmogorov-Arnold Representations correspond to two-layer KANs. To the best of our knowledge, there is not yet a “generalized” version of the theorem that corresponds to deeper KANs.

The breakthrough came when we noticed the analogy between MLPs and KANs. In MLPs, once we define a layer (which is composed of a linear transformation and non-linearities), we could stack more layers to make the network deeper. To build deep KANs, we should first answer: “what is a KAN layer?” It turns out that a KAN layer with n<sub>in</sub>\-dimensional inputs and n<sub>out</sub>\-dimensional outputs can be defined as a matrix of 1D functions:

> (2.2) Φ = {ϕ<sub>q,p</sub>}, p = 1, 2, · · · , n<sub>in</sub>, q = 1, 2 · · · , n<sub>out</sub>,

...where the functions `ϕ<sub>q,p</sub>` have trainable parameters. In the Kolmogorov-Arnold Theorem, the inner functions form a KAN layer with `n<sub>in</sub> = n` and `n<sub>out</sub> = 2n + 1`, and the outer functions form a KAN layer with `n<sub>in</sub> = 2n + 1` and `n<sub>out</sub> = 1`. So the Kolmogorov-Arnold Representations in the original (2.1) equation are simply compositions of two KAN layers. Now it becomes clear what it means to have deeper Kolmogorov-Arnold Representations: simply stack more KAN layers!

## Notations.

Let's introduce some notation. The shape of a KAN is represented by an integer array:

> (2.3) \[n<sub>0</sub> , n<sub>1</sub> , ··· , n<sub>L</sub> \],

...where n<sub>i</sub> is the number of nodes in the i<sup>th</sup> layer of the computational graph.

We denote the i<sup>th</sup> neuron in the l<sup>th</sup> layer by (l, i), and the activation value of the (l, i)-neuron by x<sub>l,i</sub>. Between layer l and layer l + 1, there are n<sub>l</sub>n<sub>l</sub>+1 activation functions: the activation function that connects (l, i) and (l + 1, j) is denoted by:

> (2.4) ϕ<sub>l,j,i</sub> , l = 0, ··· , L − 1, i = 1, ··· , n<sub>l</sub> , j = 1, ··· , n<sub>l+1</sub>.

The pre-activation of ϕ<sub>l,j,i</sub> is simply x<sub>l,i</sub>; the post-activation of ϕ<sub>l,j,i</sub> is denoted by x̃<sub>l,j,i</sub> ≡ ϕ<sub>l,j,i</sub> (x<sub>l,i</sub>). The activation value of the (l + 1, j) neuron is simply the sum of all incoming post- activations. A general KAN network is a composition of L layers: given an input vector x<sub>0</sub> ∈ R<sup>n0</sup>, the output of KAN is:

> (2.5) Omitted.  
> (2.6) Omitted.  
> (2.7) KAN(x) = (Φ<sub>L−1</sub> ◦ Φ<sub>L−2</sub> ◦ ··· ◦ Φ<sub>1</sub> ◦ Φ<sub>0</sub>)x.

We can also rewrite the above equation to make it more analogous to the original equation which is quite cumbersome. In contrast, our abstraction of KAN layers and their visualizations are cleaner and intuitive. The original Kolmogorov-Arnold Representation corresponds to a 2-Layer KAN with shape \[n, 2n + 1, 1\]. Notice that all the operations are differentiable, so we can train KANs with back propagation.

For comparison, an MLP can be written as interleaving of affine transformations W and non-linearities σ:

> (2.8) Omitted.  
> (2.9) MLP(x) = (W<sub>L−1</sub> ◦ σ ◦ W<sub>L−2</sub> ◦ σ ◦ ··· ◦ W<sub>1</sub> ◦ σ ◦ W<sub>0</sub>)x.

It is clear that MLPs treat linear transformations and non-linearities separately as W and σ, while KANs treat them all together in Φ.

## Implementation Details.

Although a KAN layer (2.7) looks extremely simple, it is non-trivial to make it well optimizable. The key tricks are as follows.

### 1\. Residual Activation Functions.

We include a basis function b(x) (similar to residual connections) such that the activation function ϕ(x) is the sum of the basis function b(x) and the spline function:

> (2.10) ϕ(x) = w<sub>b</sub>b(x) + w<sub>s</sub> spline(x).

We set:

> (2.11) b(x) = silu(x) = x/(1 + e<sup>−x</sup>)

...in most cases. spline(x) is parametrized as a linear combination of B-splines where cis are trainable as per (2.2):

> (2.12) Omitted.

In principle w<sub>b</sub> and w<sub>s</sub> are redundant since it can be absorbed into b(x) and spline(x). However, we still include these factors (which are by default trainable) to better control the overall magnitude of the activation function.

### 2\. Initialization Scales.

Each activation function is initialized to have w<sub>s</sub> = 1 and spline(x) ≈ 0. This is done by drawing B-spline coefficients c<sub>i</sub> ∼ N (0, σ<sup>2</sup>) with a small σ, typically we set σ = 0.1.

w<sub>b</sub> is initialized according to the Xavier initialization, which has been used to initialize linear layers in MLPs.

### 3\. Update of Spline Grids.

We update each grid on the fly according to its input activations, to address the issue that splines are defined on bounded regions but activation values can evolve out of the fixed region during training. Other possibilities are: (a) the grid is learnable with gradient descent, (b) use normalization such that the input range is fixed. We tried (b) at first but its performance is inferior to our current approach.

## Parameter Count.

For simplicity, let us assume a network:

1. of depth L,
    
2. with layers of equal width n<sub>0</sub> = n<sub>1</sub> = · · · = n<sub>L</sub> = N,
    
3. with each spline of order k (usually k = 3) on G intervals (for G + 1 grid points).
    

Then there are in total O(N<sup>2</sup> L(G + k)) ∼ O(N<sup>2</sup> LG) parameters. In contrast, an MLP with depth L and width N only needs O(N<sup>2</sup>L) parameters, which appears to be more efficient than KAN. Fortunately, KANs usually require much smaller N than MLPs, which not only saves parameters, but also achieves better generalization (see e.g., Figure 3.1 and 3.3) and facilitates interpretability. We remark that for 1D problems, we can take N = L = 1 and the KAN network in our implementation is nothing but a spline approximation. For higher dimensions, we characterize the generalization behaviour of KANs with a theorem below.

## KAN’s Approximation Abilities and Scaling Laws.

Continued near the top of page 7.

---

# The Results.

Using FPGAs and FPAAs.

Using Float Gate Transistors as Analog Memory Cells.

Using Synthetic Training Data and Axioms.

\[pending\]

---

# In Conclusion.

\[pending\]

Until next time: Be safe, be kind, be awesome.